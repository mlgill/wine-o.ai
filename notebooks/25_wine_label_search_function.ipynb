{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "\n",
    "from image_features import feature_detect_extract\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_SIFT_keypoints(filepath, width=320, eps=1e-7):\n",
    "    \"\"\"Extract SIFT keypoints and features\n",
    "       Given a string containing the path to an image,\n",
    "       determine the location of the keypoints and features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create detector and descriptor\n",
    "    feat_detector = cv2.FeatureDetector_create('SIFT')\n",
    "    feat_extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
    "    \n",
    "    kps, features = feature_detect_extract(filepath, \n",
    "                                           feat_detector, \n",
    "                                           feat_extractor,\n",
    "                                           width=width,\n",
    "                                           eps=eps)\n",
    "    \n",
    "    kp = np.array([x.pt for x in kps])\n",
    "    \n",
    "    return kp, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358, 128), (358, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the function\n",
    "\n",
    "in_path = '../priv/images/snooth_dot_com_6799.png'\n",
    "kp, features = get_SIFT_keypoints(in_path)\n",
    "\n",
    "features.shape, kp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map query image keypoints to K-Means clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_feature_to_clusters(features, nclusters=1500):\n",
    "    \"\"\"Map features to K-Means clusters\n",
    "       Given an input set of features and number of clusters\n",
    "       return a histogram of features mapped to an\n",
    "       existing K-Means clustering model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the k-means clusters\n",
    "    kmeans_path = '../priv/data/kmeans.h5'\n",
    "    km = h5py.File(kmeans_path, 'r')\n",
    "    km_matrix = km[str(nclusters)]\n",
    "\n",
    "    # Pairwise euclidean distances\n",
    "    ec = euclidean_distances(features, km_matrix)\n",
    "    km.close()\n",
    "\n",
    "    # Closest cluster id and count\n",
    "    closest_clust_id = np.argmin(ec, axis=1)\n",
    "    cluster_id, word_count = np.unique(closest_clust_id, return_counts=True)\n",
    "\n",
    "    # Dense matrix of word counts\n",
    "    hist = np.zeros(nclusters, dtype=np.int)\n",
    "    hist[cluster_id] = word_count\n",
    "    \n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 2, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the features to K-Means clusters\n",
    "\n",
    "query_hist = map_feature_to_clusters(features)\n",
    "\n",
    "query_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get candidate histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_candidate_histograms(query_hist, max_images=200, nclusters=1500):\n",
    "    \"\"\"Given an input histogram from a query image,\n",
    "       find the histograms of the top-N (max_images) \n",
    "    \"\"\"\n",
    "\n",
    "    # Load the inverted index data\n",
    "    index_file = '../priv/data/inverted_index.h5'\n",
    "    ix = pd.HDFStore(index_file, 'r')\n",
    "    inverted_index = ix[str(nclusters)]\n",
    "    ix.close()\n",
    "    \n",
    "    # Find all files that have keypoints mapped to\n",
    "    # non-zero parts of the query histogram and\n",
    "    # sort files by number of appearances\n",
    "    index_bins = np.nonzero(query_hist)[0]\n",
    "\n",
    "    sorted_counts = (inverted_index\n",
    "                     .loc[index_bins]\n",
    "                     .groupby('file')\n",
    "                     .sum()\n",
    "                     .sort_values('count', ascending=False)\n",
    "                     .reset_index())\n",
    "\n",
    "    # Find the top (max_images) files, accounting for ties at the\n",
    "    # cut-off point\n",
    "    max_occurrences = sorted_counts.iloc[max_images]['count']\n",
    "    \n",
    "    # Get a list of these image names\n",
    "    candidate_images = (sorted_counts\n",
    "                        .query('count >= {}'.format(max_occurrences))\n",
    "                        ['file']\n",
    "                        .values)\n",
    "    \n",
    "    # Extract the histgrams for these images\n",
    "    hist_file = '../priv/data/hist.h5'\n",
    "    hs = pd.HDFStore(hist_file, 'r')\n",
    "    data_hist = hs[str(nclusters)].set_index('image_path')\n",
    "    hs.close()\n",
    "\n",
    "    candidate_hist = data_hist.loc[candidate_images]\n",
    "    \n",
    "    return candidate_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1490</th>\n",
       "      <th>1491</th>\n",
       "      <th>1492</th>\n",
       "      <th>1493</th>\n",
       "      <th>1494</th>\n",
       "      <th>1495</th>\n",
       "      <th>1496</th>\n",
       "      <th>1497</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>snooth_dot_com_43807.jpeg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snooth_dot_com_14543.jpeg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snooth_dot_com_14563.jpeg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snooth_dot_com_11833.jpeg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snooth_dot_com_30551.jpeg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0     1     2     3     4     5     6     7     \\\n",
       "image_path                                                                  \n",
       "snooth_dot_com_43807.jpeg     0     0    23     0     0     0     0    38   \n",
       "snooth_dot_com_14543.jpeg     0     0     2     0     0     0     0     7   \n",
       "snooth_dot_com_14563.jpeg     0     0     2     0     0     0     0     7   \n",
       "snooth_dot_com_11833.jpeg     0     0     3     0     0     0     0    74   \n",
       "snooth_dot_com_30551.jpeg     0     0     8     0     0     0     0    13   \n",
       "\n",
       "                           8     9     ...   1490  1491  1492  1493  1494  \\\n",
       "image_path                             ...                                  \n",
       "snooth_dot_com_43807.jpeg     0     0  ...      0     0     0     0     0   \n",
       "snooth_dot_com_14543.jpeg     0     0  ...      0     0     0     0     0   \n",
       "snooth_dot_com_14563.jpeg     0     0  ...      0     0     0     0     0   \n",
       "snooth_dot_com_11833.jpeg     0     0  ...      0     0     0     0     0   \n",
       "snooth_dot_com_30551.jpeg     0     0  ...      0     0     0     0     0   \n",
       "\n",
       "                           1495  1496  1497  1498  1499  \n",
       "image_path                                               \n",
       "snooth_dot_com_43807.jpeg     0     0     0     8     0  \n",
       "snooth_dot_com_14543.jpeg     0     5     0     5     0  \n",
       "snooth_dot_com_14563.jpeg     0     5     0     5     0  \n",
       "snooth_dot_com_11833.jpeg     0     1     0     6     0  \n",
       "snooth_dot_com_30551.jpeg     0     4     0     8     0  \n",
       "\n",
       "[5 rows x 1500 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the candidate histograms\n",
    "\n",
    "candidate_hist = get_candidate_histograms(query_hist)\n",
    "\n",
    "candidate_hist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without IDF, and using chi2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_candidate_chi2(hist, candidate_hist):\n",
    "    \"\"\"Given a query histogram and a set of histograms\n",
    "       from candidate images, calculate the chi-squared\n",
    "       score\n",
    "    \"\"\"\n",
    "\n",
    "    chi_sq_val = ((candidate_hist - hist).pow(2) / \n",
    "                  (candidate_hist + hist + 1.0e-10)).sum(axis=1)*0.5\n",
    "    \n",
    "    return chi_sq_val.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_path\n",
       "snooth_dot_com_6799.png        0.000000\n",
       "snooth_dot_com_48533.jpeg    194.621318\n",
       "snooth_dot_com_23253.jpeg    201.852366\n",
       "snooth_dot_com_10093.jpeg    206.491596\n",
       "snooth_dot_com_11818.jpeg    206.737771\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the chi2 calculation\n",
    "\n",
    "chi_sq_val = calc_candidate_chi2(query_hist, candidate_hist)\n",
    "\n",
    "chi_sq_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With IDF and cosine distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_candidate_idf_cos(hist, candidate_hist):\n",
    "    \"\"\"Given a query histogram and a set of histograms\n",
    "       from candidate images, calculate the IDF-weighted\n",
    "       cosine distance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the inverse document frequency\n",
    "    nimages = candidate_hist.shape[0]\n",
    "    idf = np.log(nimages/(1.0 + candidate_hist.sum(axis=0).values))\n",
    "\n",
    "    # IDF-weighted cosine distances\n",
    "    idf_cos_val = pd.Series(np.squeeze(cosine_distances((candidate_hist * idf).values, \n",
    "                                                        (hist.reshape(1,-1) * idf))),\n",
    "                            index=candidate_hist.index)\n",
    "\n",
    "    return idf_cos_val.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_path\n",
       "snooth_dot_com_6799.png      3.330669e-16\n",
       "snooth_dot_com_11008.jpeg    4.268686e-01\n",
       "snooth_dot_com_1644.jpeg     4.466822e-01\n",
       "snooth_dot_com_26668.jpeg    4.781797e-01\n",
       "snooth_dot_com_2899.jpeg     4.782433e-01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the IDF weighted cosine distances\n",
    "idf_cos_val = calc_candidate_idf_cos(query_hist, candidate_hist)\n",
    "\n",
    "idf_cos_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the overlap in the top 50 images with the two techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(chi_sq_val.iloc[:50].index).intersection(idf_cos_val.iloc[:50].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With RANSAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ransac_matches(kp, features, candidate_image_list, \n",
    "                       ratio=0.7, min_matches=20):\n",
    "    \n",
    "    \"\"\"Given keypoints and features for a query image\n",
    "       plus a list of candidate images, run RANSAC\n",
    "       and find the best match\n",
    "    \"\"\"\n",
    "        \n",
    "    # The feature data\n",
    "    st = pd.HDFStore('../priv/data/features.h5', 'r')\n",
    "\n",
    "    # Euclidean matcher for comparing data\n",
    "    desc_matcher = cv2.DescriptorMatcher_create('BruteForce')\n",
    "\n",
    "    # Store the RANSAC scores here\n",
    "    score_list = list()\n",
    "\n",
    "    for candidate_image in candidate_image_list:\n",
    "\n",
    "        # Get the basename for the file\n",
    "        candidate_basename = os.path.splitext(os.path.basename(candidate_image))[0]\n",
    "\n",
    "        # Find the row index location of this file to get keypoints\n",
    "        candidate_loc = (st['basename'] == candidate_basename).idxmax()\n",
    "\n",
    "        # The beginning and ending index of keypoints/features for the candidate image\n",
    "        idx0, idx1 = st['index'].loc[candidate_loc]\n",
    "\n",
    "        # The keypoints and features for the candidate image\n",
    "        kp_ = st.select('keypoints', start=idx0, stop=idx1).values\n",
    "        features_ = st.select('features', start=idx0, stop=idx1).values\n",
    "\n",
    "        # Run brute force KNN matching with Euclidean distance to pair the features up\n",
    "        matches = desc_matcher.knnMatch(features_, features, 2)\n",
    "\n",
    "        # Extract the index for the train and candidate \n",
    "        match_list = [[x[0].trainIdx, x[0].queryIdx] \n",
    "                       for x in matches \n",
    "                       if ((len(x) >= 2) & (x[0].distance < x[1].distance*ratio))]\n",
    "        \n",
    "        filtered_matches = np.array(match_list)\n",
    "\n",
    "        if len(filtered_matches) >= min_matches:\n",
    "            # Point index for the query image\n",
    "            pts  = np.array([kp[filtered_matches[:, 0]]], dtype=np.float32)\n",
    "\n",
    "            # Point index for the candidate image\n",
    "            pts_ = np.array([kp_[filtered_matches[:, 1]]], dtype=np.float32)\n",
    "\n",
    "            # Run RANSAC - this outputs 0,1 depending on match\n",
    "            _, status = cv2.findHomography(pts, \n",
    "                                           pts_,\n",
    "                                           cv2.RANSAC,\n",
    "                                           4.0)\n",
    "\n",
    "            score_list.append((np.mean(status), candidate_basename))\n",
    "\n",
    "    st.close()\n",
    "\n",
    "    return sorted(score_list, reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 'snooth_dot_com_6799')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ransac_matches(kp, features, chi_sq_val.iloc[:100].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I'm a little surprised RANSAC only returns one match as meeting the matching criteria. May need to tune parameters if this is too strict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_wine_label(image_path, ncluster=1500):\n",
    "    # Required for calling this from an external notebook\n",
    "    # fix when this becomes a function\n",
    "    import time\n",
    "    \n",
    "    # Run SIFT\n",
    "    kp, features = get_SIFT_keypoints(image_path)\n",
    "\n",
    "    # Time the matchine\n",
    "    begin = time.time()\n",
    "    \n",
    "    # Get the cluster histogram for the image\n",
    "    query_hist = map_feature_to_clusters(features, nclusters=ncluster)\n",
    "\n",
    "    # Load histograms of similar images and rank them\n",
    "    candidate_hist = get_candidate_histograms(query_hist, nclusters=ncluster)\n",
    "    chi_sq_val = calc_candidate_chi2(query_hist, candidate_hist)\n",
    "\n",
    "    # Run RANSAC on some of them\n",
    "    matched_file = get_ransac_matches(kp, features, chi_sq_val.index[:100])\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = end - begin\n",
    "    \n",
    "    if len(matched_file) > 0:\n",
    "        matched_file = matched_file[0][1]\n",
    "    \n",
    "    # TODO: load image when used for actual search?\n",
    "    return matched_file, total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_path = '../priv/images/snooth_dot_com_6799.png'\n",
    "\n",
    "match_wine_label(image_path, 1500)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
